<html>
<head>
<!--#include virtual="/css-scroll.txt"-->
</head>
<body>
<!--#include virtual="/nav-bar.txt"-->
</body> <head>
  <!--#include virtual="/css-scroll.txt"-->
<style>
. {font-family:verdana,helvetica,sans-serif}
a {text-decoration:none;color:#3030a0}
</style>
</head> 
<body>
<div id="content">
<h2>Policy-Gradients for PSRs and POMDPs</h2>
<p><i><b>Douglas Aberdeen, Olivier Buffet, Owen Thomas</b></i>;
JMLR W&P 2:3-10, 2007.
<h3>Abstract</h3>

In uncertain and partially observable environments control policies must be a function of the complete history of actions and observations. Rather than present an ever growing history to a learner, we instead track sufficient statistics of the history and map those to a control policy. The mapping has typically been done using dynamic programming, requiring large amounts of memory. We present a general approach to mapping sufficient statistics directly to control policies by combining the tracking of sufficient statistics with the use of policy-gradient reinforcement learning. The best known sufficient statistic is the belief state, computed from a known or estimated partially observable Markov decision process (POMDP) model. More recently, predictive state representations (PSRs) have emerged as a potentially compact model of partially observable systems. Our experiments explore the usefulness of both of these sufficient statistics, exact and estimated, in direct policy-search.

</div>
 <!--#include virtual="/nav-bar.txt"--> 
</body>
<p><center>Page last modified on Sat Oct 27 18:32:46 BST 2007.</center>
<body>
<p> 

<table width="100%"> <tr>
<td align="left"><font size="-1"><i><a
href="javascript:GoAddress('webmaster','jmlr.org');">webmaster<img
src=/images/atr.gif border=0>jmlr.org</a></i></font></td> <td
align="right"><font size="-1">Copyright &copy <a target="_top"
href="http://www.jmlr.org">JMLR</a> 2000.  All rights
reserved.</font></td> </tr> </table>
</body>