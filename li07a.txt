<h2>A Bayesian Divergence Prior for Classiffier Adaptation</h2>
<p><i><b>Xiao Li, Jeff Bilmes</b></i>;
JMLR W&P 2:275-282, 2007.
<h3>Abstract</h3>

Adaptation of statistical classifiers is critical when a target (or testing) distribution is different from the distribution that governs training data. In such cases, a classifier optimized for the training distribution needs to be adapted for optimal use in the target distribution. This paper presents a Bayesian "divergence prior" for generic classifier adaptation. Instantiations of this prior lead to simple yet principled adaptation strategies for a variety of classifiers, which yield superior performance in practice. In addition, this paper derives several adaptation error bounds by applying the divergence prior in the PAC-Bayesian setting.

