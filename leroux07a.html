<html>
<head>
<!--#include virtual="/css-scroll.txt"-->
</head>
<body>
<!--#include virtual="/nav-bar.txt"-->
</body> <head>
  <!--#include virtual="/css-scroll.txt"-->
<style>
. {font-family:verdana,helvetica,sans-serif}
a {text-decoration:none;color:#3030a0}
</style>
</head> 
<body>
<div id="content">
<h2>Continuous Neural Networks</h2>
<p><i><b>Nicolas Le Roux, Yoshua Bengio</b></i>;
JMLR W&P 2:404-411, 2007.
<h3>Abstract</h3>

This article extends neural networks to the case of an uncountable number of hidden units, in several ways. In the first approach proposed, a finite parametrization is possible, allowing gradient-based learning. While having the same number of parameters as an ordinary neural network, its internal structure suggests that it can represent some smooth functions much more compactly. Under mild assumptions, we also find better error bounds than with ordinary neural networks. Furthermore, this parametrization may help reducing the problem of saturation of the neurons. In a second approach, the input-to-hidden weights are fully nonparametric, yielding a kernel machine for which we demonstrate a simple kernel formula. Interestingly, the resulting kernel machine can be made hyperparameter-free and still generalizes in spite of an absence of explicit regularization.

</div>
 <!--#include virtual="/nav-bar.txt"--> 
</body>
<p><center>Page last modified on Sat Oct 27 18:32:47 BST 2007.</center>
<body>
<p> 

<table width="100%"> <tr>
<td align="left"><font size="-1"><i><a
href="javascript:GoAddress('webmaster','jmlr.org');">webmaster<img
src=/images/atr.gif border=0>jmlr.org</a></i></font></td> <td
align="right"><font size="-1">Copyright &copy <a target="_top"
href="http://www.jmlr.org">JMLR</a> 2000.  All rights
reserved.</font></td> </tr> </table>
</body>