---
title: SVM versus Least Squares SVM
abstract: We study the relationship between Support Vector Machines (SVM) and Least
  Squares SVM (LS-SVM). Our main result shows that under mild conditions, LS-SVM for
  binaryclass classifications is equivalent to the hard margin SVM based on the well-known
  Mahalanobis distance measure. We further study the asymptotics of the hard margin
  SVM when the data dimensionality tends to infinity with a fixed sample size. Using
  recently developed theory on the asymptotics of the distribution of the eigenvalues
  of the covariance matrix, we show that under mild conditions, the equivalence result
  holds for the traditional Euclidean distance measure. These equivalence results
  are further extended to the multi-class case. Experimental results confirm the presented
  theoretical analysis.
pdf: http://proceedings.mlr.press/v2/ye07a/ye07a.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: ye07a
month: 0
tex_title: SVM versus Least Squares SVM
firstpage: 644
lastpage: 651
page: 644-651
sections: 
author:
- given: Jieping
  family: Ye
- given: Tao
  family: Xiong
date: 2007-03-11
address: San Juan, Puerto Rico
publisher: PMLR
container-title: Proceedings of the Eleventh International Conference on Artificial
  Intelligence and Statistics
volume: '2'
genre: inproceedings
issued:
  date-parts:
  - 2007
  - 3
  - 11
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
