---
title: Ellipsoidal Machines
abstract: A novel technique is proposed for improving the standard Vapnik-Chervonenkis
  (VC) dimension estimate for the Support Vector Machine (SVM) framework. The improved
  VC estimates are based on geometric arguments. By considering bounding ellipsoids
  instead of the usual bounding hyperspheres and assuming gap-tolerant classifiers,
  a linear classifier with a given margin is shown to shatter fewer points than previously
  estimated. This improved VC estimation method directly motivates a different estimator
  for the parameters of a linear classifier. Surprisingly, only VC-based arguments
  are needed to justify this modification to the SVM. The resulting technique is implemented
  using Semidefinite Programming (SDP) and is solvable in polynomial time. The new
  linear classifier also ensures certain invariances to affine transformations on
  the data which a standard SVM does not provide. We demonstrate that the technique
  can be kernelized via extensions to Hilbert spaces. Promising experimental results
  are shown on several standardized datasets.
pdf: http://proceedings.mlr.press/v2/shivaswamy07a/shivaswamy07a.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: shivaswamy07a
month: 0
firstpage: '484'
lastpage: '491'
page: 484-491
sections: 
author:
- given: Pannagadatta K.
  family: Shivaswamy
- given: Tony
  family: Jebara
date: 2007-03-11
address: San Juan, Puerto Rico
publisher: PMLR
container-title: Proceedings of the Eleventh International Conference on Artificial
  Intelligence and Statistics
volume: '2'
genre: inproceedings
issued:
  date-parts:
  - 2007
  - 3
  - 11
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
