---
title: Continuous Neural Networks
abstract: This article extends neural networks to the case of an uncountable number
  of hidden units, in several ways. In the first approach proposed, a finite parametrization
  is possible, allowing gradient-based learning. While having the same number of parameters
  as an ordinary neural network, its internal structure suggests that it can represent
  some smooth functions much more compactly. Under mild assumptions, we also find
  better error bounds than with ordinary neural networks. Furthermore, this parametrization
  may help reducing the problem of saturation of the neurons. In a second approach,
  the input-to-hidden weights are fully nonparametric, yielding a kernel machine for
  which we demonstrate a simple kernel formula. Interestingly, the resulting kernel
  machine can be made hyperparameter-free and still generalizes in spite of an absence
  of explicit regularization.
pdf: "./leroux07a/leroux07a.pdf"
layout: inproceedings
id: leroux07a
month: 0
firstpage: 404
lastpage: 411
page: 404-411
origpdf: http://jmlr.org/proceedings/papers/v2/leroux07a/leroux07a.pdf
sections: 
author:
- given: Nicolas Le
  family: Roux
- given: Yoshua
  family: Bengio
date: '2007-03-11 00:06:44'
publisher: PMLR
---
