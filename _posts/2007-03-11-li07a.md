---
title: A Bayesian Divergence Prior for Classiffier Adaptation
abstract: Adaptation of statistical classifiers is critical when a target (or testing)
  distribution is different from the distribution that governs training data. In such
  cases, a classifier optimized for the training distribution needs to be adapted
  for optimal use in the target distribution. This paper presents a Bayesian ``divergence
  prior'' for generic classifier adaptation. Instantiations of this prior lead to
  simple yet principled adaptation strategies for a variety of classifiers, which
  yield superior performance in practice. In addition, this paper derives several
  adaptation error bounds by applying the divergence prior in the PAC-Bayesian setting.
pdf: http://proceedings.pmlr.press/li07a/li07a.pdf
layout: inproceedings
id: li07a
month: 0
firstpage: 275
lastpage: 282
page: 275-282
origpdf: http://jmlr.org/proceedings/papers/v2/li07a/li07a.pdf
sections: 
author:
- given: Xiao
  family: Li
- given: Jeff
  family: Bilmes
date: 2007-03-11
publisher: PMLR
---
