---
title: A Bayesian Divergence Prior for Classiffier Adaptation
abstract: Adaptation of statistical classifiers is critical when a target (or testing)
  distribution is different from the distribution that governs training data. In such
  cases, a classifier optimized for the training distribution needs to be adapted
  for optimal use in the target distribution. This paper presents a Bayesian “divergence
  prior” for generic classifier adaptation. Instantiations of this prior lead to simple
  yet principled adaptation strategies for a variety of classifiers, which yield superior
  performance in practice. In addition, this paper derives several adaptation error
  bounds by applying the divergence prior in the PAC-Bayesian setting.
pdf: http://proceedings.mlr.press/v2/li07a/li07a.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: li07a
month: 0
firstpage: '275'
lastpage: '282'
page: 275-282
sections: 
author:
- given: Xiao
  family: Li
- given: Jeff
  family: Bilmes
date: 2007-03-11
address: San Juan, Puerto Rico
publisher: PMLR
container-title: Proceedings of the Eleventh International Conference on Artificial
  Intelligence and Statistics
volume: '2'
genre: inproceedings
issued:
  date-parts:
  - 2007
  - 3
  - 11
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
