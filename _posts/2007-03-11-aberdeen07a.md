---
title: Policy-Gradients for PSRs and POMDPs
abstract: In uncertain and partially observable environments control policies must
  be a function of the complete history of actions and observations. Rather than present
  an ever growing history to a learner, we instead track sufficient statistics of
  the history and map those to a control policy. The mapping has typically been done
  using dynamic programming, requiring large amounts of memory. We present a general
  approach to mapping sufficient statistics directly to control policies by combining
  the tracking of sufficient statistics with the use of policy-gradient reinforcement
  learning. The best known sufficient statistic is the belief state, computed from
  a known or estimated partially observable Markov decision process (POMDP) model.
  More recently, predictive state representations (PSRs) have emerged as a potentially
  compact model of partially observable systems. Our experiments explore the usefulness
  of both of these sufficient statistics, exact and estimated, in direct policy-search.
pdf: http://jmlr.org/proceedings/papers/v2/aberdeen07a/aberdeen07a.pdf
layout: inproceedings
id: aberdeen07a
month: 0
firstpage: 3
lastpage: 10
page: 3-10
sections: 
author:
- given: Douglas
  family: Aberdeen
- given: Olivier
  family: Buffet
- given: Owen
  family: Thomas
reponame: v2
date: 2007-03-11
address: San Juan, Puerto Rico
publisher: PMLR
container-title: Proceedings of the Eleventh International Conference on Artificial
  Intelligence and Statistics
volume: '2'
genre: inproceedings
issued:
  date-parts:
  - 2007
  - 3
  - 11
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
