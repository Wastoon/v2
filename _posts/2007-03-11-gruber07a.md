---
title: Hidden Topic Markov Models
abstract: Algorithms such as Latent Dirichlet Allocation (LDA) have achieved significant
  progress in modeling word document relationships. These algorithms assume each word
  in the document was generated by a hidden topic and explicitly model the word distribution
  of each topic as well as the prior distribution over topics in the document. Given
  these parameters, the topics of all words in the same document are assumed to be
  independent. In this paper, we propose modeling the topics of words in the document
  as a Markov chain. Specifically, we assume that all words in the same sentence have
  the same topic, and successive sentences are more likely to have the same topics.
  Since the topics are hidden, this leads to using the well-known tools of Hidden
  Markov Models for learning and inference. We show that incorporating this dependency
  allows us to learn better topics and to disambiguate words that can belong to different
  topics. Quantitatively, we show that we obtain better perplexity in modeling documents
  with only a modest increase in learning and inference complexity.
pdf: http://proceedings.pmlr.press/gruber07a/gruber07a.pdf
layout: inproceedings
id: gruber07a
month: 0
firstpage: 163
lastpage: 170
page: 163-170
origpdf: http://jmlr.org/proceedings/papers/v2/gruber07a/gruber07a.pdf
sections: 
author:
- given: Amit
  family: Gruber
- given: Yair
  family: Weiss
- given: Michal
  family: Rosen-Zvi
date: 2007-03-11
address: San Juan, Puerto Rico
publisher: PMLR
container-title: Proceedings of the Eleventh International Conference on Artificial
  Intelligence and Statistics
volume: '2'
genre: inproceedings
issued:
  date-parts:
  - 2007
  - 3
  - 11
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
