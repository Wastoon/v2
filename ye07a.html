<html>
<head>
<!--#include virtual="/css-scroll.txt"-->
</head>
<body>
<!--#include virtual="/nav-bar.txt"-->
</body> <head>
  <!--#include virtual="/css-scroll.txt"-->
<style>
. {font-family:verdana,helvetica,sans-serif}
a {text-decoration:none;color:#3030a0}
</style>
</head> 
<body>
<div id="content">
<h2>SVM versus Least Squares SVM</h2>
<p><i><b>Jieping Ye, Tao Xiong</b></i>;
JMLR W&P 2:644-651, 2007.
<h3>Abstract</h3>

We study the relationship between Support Vector Machines (SVM) and Least Squares SVM (LS-SVM). Our main result shows that under mild conditions, LS-SVM for binaryclass classifications is equivalent to the hard margin SVM based on the well-known Mahalanobis distance measure. We further study the asymptotics of the hard margin SVM when the data dimensionality tends to infinity with a fixed sample size. Using recently developed theory on the asymptotics of the distribution of the eigenvalues of the covariance matrix, we show that under mild conditions, the equivalence result holds for the traditional Euclidean distance measure. These equivalence results are further extended to the multi-class case. Experimental results confirm the presented theoretical analysis.

</div>
 <!--#include virtual="/nav-bar.txt"--> 
</body>
<p><center>Page last modified on Sat Oct 27 18:32:48 BST 2007.</center>
<body>
<p> 

<table width="100%"> <tr>
<td align="left"><font size="-1"><i><a
href="javascript:GoAddress('webmaster','jmlr.org');">webmaster<img
src=/images/atr.gif border=0>jmlr.org</a></i></font></td> <td
align="right"><font size="-1">Copyright &copy <a target="_top"
href="http://www.jmlr.org">JMLR</a> 2000.  All rights
reserved.</font></td> </tr> </table>
</body>