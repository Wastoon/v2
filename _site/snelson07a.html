<!DOCTYPE html>
<html>
  <head>
    <link rel="alternate" type="application/rss+xml" href="http://www.jmlr.org/jmlr.xml" title="JMLR RSS">
<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />
<link rel="stylesheet" type="text/css" href="http://www.jmlr.org/style.css">

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-92432422-1', 'auto');
  ga('send', 'pageview');

</script>


  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {
        equationNumbers: {
          autoNumber: "AMS"
        }
      },
      tex2jax: {
        inlineMath: [ ['$','$'] ],
        displayMath: [ ['$$','$$'] ],
        processEscapes: true,
      }
    });
  </script>
  <script type="text/javascript"
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</script>




  </head>
  <body>

    <div id="fixed">
  <br>
  <a align="right" href="http://www.jmlr.org" target=_top><img align="right" class="jmlr" src="/img/jmlr.jpg" border="0"></a> 
  <p><br><br>
  <p align="right"> <a href="/">PMLR</a>
  <p align="right"> <a href="http://www.jmlr.org/">JMLR</a> 
  <p align="right"> <a href="http://www.jmlr.org/mloss">MLOSS</a>
  <p align="right"> <a href="/faq.html">FAQ</a>
  <p align="right"> <a href="/spec.html">Submission Format</a>
  <p><br><br>
  <p align="right"> <a href="http://proceedings.mlr.press/feed.xml"> 
      <img src="/img/RSS.gif" class="rss" alt="RSS Feed">
    </a>
</div>


    <div class="page-content">
      <div class="wrapper">
        <head>
  <meta charset="utf-8">
  <title>Local and global sparse Gaussian process approximations</title>

  
<p align="right">[<a href="https://github.com/mlresearch/v2/edit/gh-pages/_posts/2007-03-11-snelson07a.md">edit</a>]</p>



  <meta name="citation_title" content="Local and global sparse Gaussian process approximations">




<meta name="citation_publication_date" content="2007">


<meta name="citation_conference_title" content="Artificial Intelligence and Statistics">
<meta name="citation_issn" content="1938-7228">
<meta name="citation_volume" content="2">
<meta name="citation_firstpage" content="524">
<meta name="citation_lastpage" content="531">

<meta name="citation_pdf_url" content="http://proceedings.pmlr.press/snelson07a/snelson07a.pdf">


<meta name="description" content="Electronic Proceedings of Artificial Intelligence and Statistics">



        
  <meta name="viewport" content="width=device-width">

</head>

<div id="content">
  <article class="post-content">
  <h1>Local and global sparse Gaussian process approximations</h1>
  <div id="authors" class="authors">
    
    
    Edward Snelson, 

    
    Zoubin Ghahramani

    ;
  </div>
  <div id="info" class="authors">
    Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics, PMLR 2:524-531, 2007.
  </div> <!-- info -->
  
  <h2>Abstract</h2>
  <div id="abstract" class="abstract">
    Gaussian process (GP) models are flexible probabilistic nonparametric models for regression, classification and other tasks. Unfortunately they suffer from computational intractability for large data sets. Over the past decade there have been many different approximations developed to reduce this cost. Most of these can be termed global approximations, in that they try to summarize all the training data via a small set of support points. A different approach is that of local regression, where many local experts account for their own part of space. In this paper we start by investigating the regimes in which these different approaches work well or fail. We then proceed to develop a new sparse GP approximation which is a combination of both the global and local approaches. Theoretically we show that it is derived as a natural extension of the framework developed by QuiÂ onero Candela and Rasmussen [2005] for n sparse GP approximations. We demonstrate the benefits of the combined approximation on some 1D examples for illustration, and on some large real-world data sets.
  </div>
  <h2>Related Material</h2>
  <div id="extras">
    <ul>
      <li><a href="http://proceedings.pmlr.press/snelson07a/snelson07a.pdf">Download PDF</a></li>
      
    </ul>
  </div>
  <div id="content">
    

  </div>
</article>


  
  <div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    // Required: on line below, replace text in quotes with your forum shortname
    var disqus_shortname = 'mlrpress';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

  
</div>

      </div>
    </div>
    

    <body>
  <div id="content">
    <center>This site last compiled 2017-02-23 22:13:39 +0000</center>

    <table width="100%">
      <tr>
	<td align="left"><font size="-1">
	    <i><a href="https://github.com/mlresearch/v2">Github Account</a></i></font></td>
	<td align="right"><font size="-1">
	    Copyright &copy <a href="http://proceedings.mlr.press">PMLR</a> 2017. All rights reserved.</font></td>
      </tr>
    </table>
  </div>
</body>


  </body>

</html>
